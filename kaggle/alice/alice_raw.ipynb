{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set desired options\n",
    "\n",
    "from __future__ import division, print_function\n",
    "# Disable Anaconda warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function for writing predictions to a file\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Websites total: 48371\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone_le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25075</th>\n",
       "      <td>www.abmecatronique.com</td>\n",
       "      <td>com</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>groups.live.com</td>\n",
       "      <td>com</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42436</th>\n",
       "      <td>majeureliguefootball.wordpress.com</td>\n",
       "      <td>com</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30911</th>\n",
       "      <td>cdt46.media.tourinsoft.eu</td>\n",
       "      <td>eu</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8104</th>\n",
       "      <td>www.hdwallpapers.eu</td>\n",
       "      <td>eu</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     site zone  zone_le\n",
       "25075              www.abmecatronique.com  com       28\n",
       "13997                     groups.live.com  com       28\n",
       "42436  majeureliguefootball.wordpress.com  com       28\n",
       "30911           cdt46.media.tourinsoft.eu   eu       41\n",
       "8104                  www.hdwallpapers.eu   eu       41"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load websites dictionary\n",
    "with open(r\"data/site_dic.pkl\", \"rb\") as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "\n",
    "# Create dataframe for the dictionary\n",
    "sites_dict = pd.DataFrame(list(site_dict.keys()), index=list(site_dict.values()), columns=['site'])\n",
    "\n",
    "sites_dict['zone'] = sites_dict['site'].str.split('.').apply(lambda x: x[-1])\n",
    "sites_dict.loc[sites_dict['zone'].str.isnumeric(), 'zone'] = 'ip_address'\n",
    "sites_dict['zone_le'] = LabelEncoder().fit_transform(sites_dict['zone'])\n",
    "\n",
    "print(u'Websites total:', sites_dict.shape[0])\n",
    "sites_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(path, is_alice=False):\n",
    "    df_alice = pd.read_csv(path)\n",
    "    \n",
    "    # удаление подряд идущих дубликватов\n",
    "    df_alice = df_alice.loc[df_alice['site'].shift()!=df_alice['site']]\n",
    "    \n",
    "    df_alice['site1'] = df_alice['site']\n",
    "    df_alice['time1'] = df_alice['timestamp'].apply(pd.to_datetime)\n",
    "\n",
    "    del df_alice['site']\n",
    "    del df_alice['timestamp']\n",
    "\n",
    "    for i in range(-1, -10, -1):\n",
    "        df_alice['site' + str(-i+1)] = df_alice['site1'].shift(i)\n",
    "        df_alice['time' + str(-i+1)] = df_alice['time1'].shift(i)\n",
    "\n",
    "        df_alice['dt'] = (df_alice['time' + str(-i+1)] - df_alice['time1']).dt.seconds / 60\n",
    "        df_alice.loc[df_alice['dt']>DT, ['time' + str(-i+1), 'site' + str(-i+1)]] = None\n",
    "\n",
    "    del df_alice['dt']\n",
    "\n",
    "    to_int = dict(zip(sites_dict['site'], sites_dict.index))\n",
    "    for col in df_alice.columns:\n",
    "        if 'site' in col:\n",
    "            df_alice[col] = df_alice[col].map(to_int)\n",
    "    df_alice['target'] = int(is_alice)\n",
    "    df_alice['for_folds'] = df_alice.index / (df_alice.shape[0] - 1)\n",
    "    return df_alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_get_df = False\n",
    "\n",
    "if eval_get_df:\n",
    "    df_alice = get_df('data/train/Alice_log.csv', is_alice=True)\n",
    "\n",
    "    df_user_list = list()\n",
    "    dirname = r'data\\train\\other_user_logs'\n",
    "    for filename in os.listdir(dirname):\n",
    "        path = os.path.join(dirname, filename)\n",
    "        df_user = get_df(path)\n",
    "        df_user_list.append(df_user)\n",
    "\n",
    "    train_df = pd.concat([df_alice, *df_user_list], axis=0)\n",
    "    train_df.to_csv('data/new_train_upd.csv')\n",
    "else:\n",
    "    train_df = pd.read_csv('data/new_train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>270</td>\n",
       "      <td>2013-02-12 16:25:10</td>\n",
       "      <td>270.0</td>\n",
       "      <td>2013-02-12 16:25:11</td>\n",
       "      <td>270.0</td>\n",
       "      <td>2013-02-12 16:32:10</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:11</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:24</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:26</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167</td>\n",
       "      <td>2013-11-25 08:03:45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-11-25 08:03:46</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2013-11-25 08:03:50</td>\n",
       "      <td>781.0</td>\n",
       "      <td>2013-11-25 08:03:50</td>\n",
       "      <td>781.0</td>\n",
       "      <td>2013-11-25 08:03:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-11-25 08:03:52</td>\n",
       "      <td>781.0</td>\n",
       "      <td>2013-11-25 08:03:53</td>\n",
       "      <td>781.0</td>\n",
       "      <td>2013-11-25 08:03:54</td>\n",
       "      <td>781.0</td>\n",
       "      <td>2013-11-25 08:03:55</td>\n",
       "      <td>270.0</td>\n",
       "      <td>2013-11-25 08:03:58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>820</td>\n",
       "      <td>2013-11-19 13:25:23</td>\n",
       "      <td>820.0</td>\n",
       "      <td>2013-11-19 13:25:25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>8675.0</td>\n",
       "      <td>2013-12-17 13:35:27</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-13 13:33:01</td>\n",
       "      <td>820.0</td>\n",
       "      <td>2014-02-13 13:33:04</td>\n",
       "      <td>820.0</td>\n",
       "      <td>2014-02-13 13:33:27</td>\n",
       "      <td>820.0</td>\n",
       "      <td>2014-02-13 13:33:29</td>\n",
       "      <td>1446.0</td>\n",
       "      <td>2014-02-13 13:41:50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-12 16:52:26</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-04-12 16:52:33</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-04-12 16:52:45</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2013-04-12 16:52:45</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2013-04-12 16:52:48</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-04-12 16:53:09</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2013-04-12 16:53:09</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2013-04-12 16:53:09</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2013-04-12 16:53:10</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-04-12 16:54:10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-11-16 10:32:07</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-11-16 10:32:21</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2013-11-16 10:32:23</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-11-16 10:32:37</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2013-11-16 10:32:37</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-11-16 10:32:38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-11-16 10:33:35</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-11-16 10:33:43</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-11-16 10:33:44</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-11-16 10:33:45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   site1               time1  site2               time2  site3  \\\n",
       "0    270 2013-02-12 16:25:10  270.0 2013-02-12 16:25:11  270.0   \n",
       "1    167 2013-11-25 08:03:45    1.0 2013-11-25 08:03:46   66.0   \n",
       "2    820 2013-11-19 13:25:23  820.0 2013-11-19 13:25:25    NaN   \n",
       "3      1 2013-04-12 16:52:26   21.0 2013-04-12 16:52:33   21.0   \n",
       "4      1 2013-11-16 10:32:07   21.0 2013-11-16 10:32:21   23.0   \n",
       "\n",
       "                time3  site4               time4   site5               time5  \\\n",
       "0 2013-02-12 16:32:10   21.0 2013-02-12 16:32:11    21.0 2013-02-12 16:32:24   \n",
       "1 2013-11-25 08:03:50  781.0 2013-11-25 08:03:50   781.0 2013-11-25 08:03:51   \n",
       "2                 NaT    NaN                 NaT  8675.0 2013-12-17 13:35:27   \n",
       "3 2013-04-12 16:52:45   22.0 2013-04-12 16:52:45    23.0 2013-04-12 16:52:48   \n",
       "4 2013-11-16 10:32:23   21.0 2013-11-16 10:32:37    23.0 2013-11-16 10:32:37   \n",
       "\n",
       "   ...                 time6  site7               time7   site8  \\\n",
       "0  ...   2013-02-12 16:32:25   21.0 2013-02-12 16:32:25  7832.0   \n",
       "1  ...   2013-11-25 08:03:52  781.0 2013-11-25 08:03:53   781.0   \n",
       "2  ...   2014-02-13 13:33:01  820.0 2014-02-13 13:33:04   820.0   \n",
       "3  ...   2013-04-12 16:53:09   23.0 2013-04-12 16:53:09    22.0   \n",
       "4  ...   2013-11-16 10:32:38    1.0 2013-11-16 10:33:35    21.0   \n",
       "\n",
       "                time8  site9               time9  site10              time10  \\\n",
       "0 2013-02-12 16:32:26   30.0 2013-02-12 16:32:27  7832.0 2013-02-12 16:32:27   \n",
       "1 2013-11-25 08:03:54  781.0 2013-11-25 08:03:55   270.0 2013-11-25 08:03:58   \n",
       "2 2014-02-13 13:33:27  820.0 2014-02-13 13:33:29  1446.0 2014-02-13 13:41:50   \n",
       "3 2013-04-12 16:53:09   40.0 2013-04-12 16:53:10    21.0 2013-04-12 16:54:10   \n",
       "4 2013-11-16 10:33:43   21.0 2013-11-16 10:33:44    21.0 2013-11-16 10:33:45   \n",
       "\n",
       "  target  \n",
       "0      1  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the training and test data sets\n",
    "test_df = pd.read_csv('data/test_sessions.csv',\n",
    "                      index_col='session_id')\n",
    "\n",
    "# Switch time1, ..., time10 columns to datetime type\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='for_folds')\n",
    "del train_df['for_folds']\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "# Look at the first rows of the training set\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change site1, ..., site10 columns type to integer and fill NA-values with zeros\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[sites] = train_df[sites].fillna(0).astype('int')\n",
    "test_df[sites] = test_df[sites].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our target variable\n",
    "y_train = train_df['target']\n",
    "\n",
    "# United dataframe of the initial data \n",
    "full_df = pd.concat([train_df.drop('target', axis=1), test_df])\n",
    "\n",
    "# Index to split the training and test data sets\n",
    "idx_split = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>270</td>\n",
       "      <td>270</td>\n",
       "      <td>270</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>7832</td>\n",
       "      <td>21</td>\n",
       "      <td>7832</td>\n",
       "      <td>30</td>\n",
       "      <td>7832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>781</td>\n",
       "      <td>781</td>\n",
       "      <td>781</td>\n",
       "      <td>781</td>\n",
       "      <td>781</td>\n",
       "      <td>781</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>820</td>\n",
       "      <td>820</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8675</td>\n",
       "      <td>820</td>\n",
       "      <td>820</td>\n",
       "      <td>820</td>\n",
       "      <td>820</td>\n",
       "      <td>1446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>40</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site1  site2  site3  site4  site5  site6  site7  site8  site9  site10\n",
       "0    270    270    270     21     21   7832     21   7832     30    7832\n",
       "1    167      1     66    781    781    781    781    781    781     270\n",
       "2    820    820      0      0   8675    820    820    820    820    1446\n",
       "3      1     21     21     22     23     21     23     22     40      21\n",
       "4      1     21     23     21     23     21      1     21     21      21"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe with indices of visited websites in session\n",
    "full_sites = full_df[sites]\n",
    "\n",
    "full_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time1</th>\n",
       "      <th>time2</th>\n",
       "      <th>time3</th>\n",
       "      <th>time4</th>\n",
       "      <th>time5</th>\n",
       "      <th>time6</th>\n",
       "      <th>time7</th>\n",
       "      <th>time8</th>\n",
       "      <th>time9</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-12 16:25:10</td>\n",
       "      <td>2013-02-12 16:25:11</td>\n",
       "      <td>2013-02-12 16:32:10</td>\n",
       "      <td>2013-02-12 16:32:11</td>\n",
       "      <td>2013-02-12 16:32:24</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>2013-02-12 16:32:26</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                time1               time2               time3  \\\n",
       "0 2013-02-12 16:25:10 2013-02-12 16:25:11 2013-02-12 16:32:10   \n",
       "\n",
       "                time4               time5               time6  \\\n",
       "0 2013-02-12 16:32:11 2013-02-12 16:32:24 2013-02-12 16:32:25   \n",
       "\n",
       "                time7               time8               time9  \\\n",
       "0 2013-02-12 16:32:25 2013-02-12 16:32:26 2013-02-12 16:32:27   \n",
       "\n",
       "               time10  \n",
       "0 2013-02-12 16:32:27  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df[times].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_time_diff(row): \n",
    "    time_length = row.shape[0] - 1 \n",
    "    time_diff = [0]*time_length \n",
    "    i = 0 \n",
    "    while (i < time_length)and pd.notnull(row[i+1]): \n",
    "        time_diff[i] = (row[i+1] - row[i]) / np.timedelta64(1,'s') \n",
    "        i += 1 \n",
    "    return  time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_features(df):\n",
    "    time_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    hour = df['time1'].dt.hour\n",
    "    time_df['hour'] = hour\n",
    "    time_df['day_'] = df['time1'].dt.day\n",
    "    time_df['month'] = df['time1'].dt.month\n",
    "    time_df['year'] = df['time1'].dt.year\n",
    "    \n",
    "    time_df['morning'] = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "    time_df['day'] = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    time_df['evening'] = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "#     time_df['night'] = ((hour >= 0) & (hour <= 6)).astype('int')\n",
    "\n",
    "    time_df['min'] = df['time1'] \n",
    "    time_df['max'] = df[times].max(axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "    for px in ['min', 'max']:\n",
    "        time_df['minutes'] = time_df[px].dt.hour * 60 + time_df[px].dt.minute\n",
    "        time_df['sin_'+px] = np.sin(2*np.pi*time_df['minutes']/1440.)\n",
    "        time_df['cos_'+px] = np.cos(2*np.pi*time_df['minutes']/1440.)\n",
    "\n",
    "    time_df['dow'] = time_df['min'].apply(lambda ts: ts.date().weekday())\n",
    "    time_df['weekend'] = (time_df['dow'] > 4).astype('int')\n",
    "    time_df['n_null'] = df[times].isnull().sum(axis=1)\n",
    "    \n",
    "#     time_df['mean_dt'] = time_df['dt'] / (10 - time_df['n_null'])\n",
    "    \n",
    "    \n",
    "#     time_diff = []\n",
    "#     for row in df.values:\n",
    "#         time_diff.append (get_time_diff (row))\n",
    "#     time_diff = np.log1p(np.array(time_diff).astype(float))\n",
    "#     time_names = ['time_diff'+str(j) for j in range(1,10)] \n",
    "#     for ind, column_name in enumerate(time_names): \n",
    "#         time_df[column_name] = time_diff[:,ind] \n",
    "\n",
    "    time_df['dt'] = time_df['max'] - time_df['min']\n",
    "    for time in times[1:]:\n",
    "        dt_ = (df[time] - time_df['min']).fillna(time_df['dt'])\n",
    "        time_df['dt_' + time] = np.log1p(np.abs(dt_.astype('timedelta64[s]')))\n",
    "    time_df['dt'] = np.log1p(np.abs(time_df['dt'].astype('timedelta64[s]')))\n",
    "    time_df['dt_mean'] = time_df[['dt_' + time for time in times[1:]]].mean(axis=1)\n",
    "    time_df['dt_std'] = time_df[['dt_' + time for time in times[1:]]].std(axis=1)\n",
    "    \n",
    "    s_columns = [col for col in time_df.columns if time_df[col].dtype != '<M8[ns]']\n",
    "    \n",
    "    s_scaler = StandardScaler()\n",
    "    time_df[s_columns] = s_scaler.fit_transform(time_df[s_columns])\n",
    "#     for time in times[1:]:\n",
    "#         time_df['dt_' + time] = (time_df['dt_' + time] - 3) / 3\n",
    "#     time_df['dt'] = (time_df['dt'] - 3) / 3\n",
    "#     time_df['dt_mean'] = (time_df['dt_mean'] - 3) / 3\n",
    "#     time_df['dow'] = (time_df['dow'] - 3) / 3\n",
    "#     time_df['n_null'] = (time_df['n_null'] - 4.5) / 4.5\n",
    "\n",
    "    time_df = time_df.drop(['min', 'max'], axis=1)\n",
    "    \n",
    "    for col in time_df.columns:\n",
    "        time_df[col] = time_df[col].fillna(time_df[col].mean())\n",
    "\n",
    "    return time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           sin_min       cos_max        n_null            dt          year  \\\n",
      "min  -1.483732e+00 -8.384219e-01 -2.444461e-01 -1.880744e+00 -1.592300e+00   \n",
      "mean  2.820128e-14 -4.585466e-15  2.882429e-13 -4.949774e-15  1.378052e-11   \n",
      "max   1.669230e+00  4.904801e+00  5.843342e+00  7.473835e+00  6.280224e-01   \n",
      "\n",
      "      (-1.69, -0.406]  (-0.406, 0.873]  (0.873, 2.152]  (2.152, 3.431]  \n",
      "min          0.000000         0.000000        0.000000        0.000000  \n",
      "mean         0.483626         0.349742        0.146071        0.020561  \n",
      "max          1.000000         1.000000        1.000000        1.000000  \n"
     ]
    }
   ],
   "source": [
    "full_time = get_time_features(full_df[times])\n",
    "ft_columns = ['dow', 'weekend', 'day_', 'month', 'sin_min', 'cos_min', 'dt', 'dt_std', 'dt_mean', 'n_null',\n",
    "              'dt_time10', 'dt_time9', 'dt_time8', 'dt_time7', 'dt_time6', 'dt_time5', 'dt_time4', 'dt_time3', 'dt_time2',\n",
    "              'morning', 'day', 'evening', 'sin_max', 'cos_max', 'hour', 'year', 'minutes']  # full_time.columns\n",
    "good_cols = ['sin_min', 'cos_max', 'n_null', 'dt', 'year']\n",
    "\n",
    "full_time = full_time[ft_columns]\n",
    "\n",
    "hours_dum = pd.get_dummies(pd.cut(full_time['hour'], bins=4))\n",
    "\n",
    "full_time = pd.concat([full_time[good_cols], hours_dum], axis=1)\n",
    "print(full_time.agg(['min', 'mean', 'max']))\n",
    "full_time = full_time.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "str not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-75da06d89e34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfull_sites_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_sites_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_tf_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_sites_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_tf_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[.-]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: str not found"
     ]
    }
   ],
   "source": [
    "full_sites_tf = full_sites.copy()\n",
    "\n",
    "for col in full_sites_tf.columns:\n",
    "    full_sites_tf[col] = full_sites_tf[col].map(sites_dict.site)\n",
    "\n",
    "full_sites_tf = full_sites_tf.fillna('')\n",
    "df_tf_col = full_sites_tf.apply(lambda x: '.'.join([i for i in x if len(i)>0]), axis=1)\n",
    "df_tf_col = df_tf_col.str.split('[.-]').str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(max_df=.7, sublinear_tf=True)\n",
    "df_tf = vect.fit_transform(df_tf_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "kfold_split = KFold(n_splits=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = df_tf[:idx_split,:]\n",
    "# X_test = df_tf[idx_split:,:]\n",
    "\n",
    "\n",
    "X_train = hstack([df_tf[:idx_split,:], full_time[:idx_split,:]])\n",
    "X_test = hstack([df_tf[idx_split:,:], full_time[idx_split:,:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [0.01, 0.03]\n",
    "# cw_values = [{0: 0.6, 1: 0.4}, {0: 0.9, 1: 0.1}, {0: 0.8, 1: 0.2}, {0: 0.7, 1: 0.3} , {0: 0.3, 1: 0.7}]\n",
    "\n",
    "lrcv = LogisticRegressionCV(Cs=c_values, scoring='roc_auc', n_jobs=-1, cv=kfold_split,\n",
    "                            verbose=1, class_weight='balanced', max_iter=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:  8.3min remaining: 24.9min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  8.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 56s, sys: 2min 36s, total: 6min 33s\n",
      "Wall time: 10min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lrcv.fit(X_train, y_train);\n",
    "lrcv.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC [0.01]: 0.9574+-0.0141\n",
      "ROC_AUC [0.03]: 0.9587+-0.0136\n",
      "Best params: [0.03]\n"
     ]
    }
   ],
   "source": [
    "cvr = lrcv.scores_[1]\n",
    "idx = cvr.mean(axis=0).argmax()\n",
    "for i in range(cvr.shape[1]):\n",
    "    print(f\"ROC_AUC [{lrcv.Cs_[i]:>4}]: {cvr[:, i].mean():.4f}+-{cvr[:, i].std():.4f}\")\n",
    "print(f\"Best params: {lrcv.C_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC_AUC [0.01]: 0.9574+-0.0141\n",
    "ROC_AUC [0.03]: 0.9587+-0.0136\n",
    "ROC_AUC [ 0.1]: 0.9586+-0.0127\n",
    "ROC_AUC [ 0.3]: 0.9578+-0.0123\n",
    "ROC_AUC [ 1.0]: 0.9559+-0.0129\n",
    "ROC_AUC [ 3.0]: 0.9531+-0.0142\n",
    "ROC_AUC [10.0]: 0.9488+-0.0167\n",
    "Best params: [0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_tf[:idx_split,:]\n",
    "X_test = df_tf[idx_split:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'C': 0.1, 'class_weight': 'balanced', 'random_state':17, 'n_jobs':1}\n",
    "\n",
    "logit = LogisticRegression(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 2.52 s, total: 12.8 s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv_scores = cross_val_score(logit, X_train, y_train, cv=kfold_split, \n",
    "                            scoring='roc_auc', n_jobs=-1) # hangs with n_jobs > 1, and locally this runs much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC: 0.9583+-0.0126\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROC_AUC: {cv_scores.mean():.4f}+-{cv_scores.std():.4f}\")\n",
    "# cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.fit(X_train, y_train)\n",
    "logit_test_pred = logit.predict_proba(X_test)[:, 1]\n",
    "write_to_submission_file(logit_test_pred, 'subm_raw_07.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check columns importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cw {0: 0.2, 1: 0.8}: 0.9542+-0.0159\n",
      "cw {0: 0.3, 1: 0.7}: 0.9537+-0.0162\n",
      "cw {0: 0.4, 1: 0.6}: 0.9529+-0.0165\n",
      "CPU times: user 3min 28s, sys: 22.5 s, total: 3min 51s\n",
      "Wall time: 8min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv_scores_arr = list()\n",
    "\n",
    "for cw in [{0: 0.2, 1: 0.8}, {0: 0.3, 1: 0.7}, {0: 0.4, 1: 0.6}]:\n",
    "    vect = TfidfVectorizer(max_df=0.9, sublinear_tf=True)\n",
    "    df_tf = vect.fit_transform(df_tf_col)\n",
    "    params = {'C': 0.1, 'class_weight': cw, 'random_state':17, 'n_jobs':1}\n",
    "    \n",
    "    X_train = hstack([df_tf[:idx_split,:], full_time[:idx_split,:]])\n",
    "    X_test = hstack([df_tf[idx_split:,:], full_time[idx_split:,:]])\n",
    "    \n",
    "    logit = LogisticRegression(**params)\n",
    "    cv_scores_ = cross_val_score(logit, X_train, y_train, cv=kfold_split, scoring='roc_auc', n_jobs=-1)\n",
    "    cv_scores_arr.append(cv_scores_)\n",
    "    \n",
    "    print(f\"cw {cw}: {cv_scores_.mean():.4f}+-{cv_scores_.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cw balanced  : 0.9586+-0.0127\n",
    "cw {0: 0.9, 1: 0.1}: 0.9390+-0.0209\n",
    "cw {0: 0.8, 1: 0.2}: 0.9451+-0.0188\n",
    "cw {0: 0.7, 1: 0.3}: 0.9483+-0.0178\n",
    "cw {0: 0.6, 1: 0.4}: 0.9504+-0.0172\n",
    "cw {0: 0.5, 1: 0.5}: 0.9518+-0.0168\n",
    "cw {0: 0.4, 1: 0.6}: 0.9529+-0.0165\n",
    "cw {0: 0.3, 1: 0.7}: 0.9537+-0.0162\n",
    "cw {0: 0.2, 1: 0.8}: 0.9542+-0.0159"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define good tf-idf params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sublinear_tf 0     : 0.9566+-0.0126\n",
      "CPU times: user 1min 9s, sys: 7.57 s, total: 1min 17s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {'C': 0.1, 'class_weight': 'balanced', 'random_state':17, 'n_jobs':1}\n",
    "cv_scores_arr = list()\n",
    "\n",
    "for i in [False]:\n",
    "    vect = TfidfVectorizer(max_df=0.9, sublinear_tf=False)\n",
    "    df_tf = vect.fit_transform(df_tf_col)\n",
    "    \n",
    "    X_train = hstack([df_tf[:idx_split,:], full_time[:idx_split,:]])\n",
    "    X_test = hstack([df_tf[idx_split:,:], full_time[idx_split:,:]])\n",
    "    \n",
    "    logit = LogisticRegression(**params)\n",
    "    cv_scores_ = cross_val_score(logit, X_train, y_train, cv=kfold_split, scoring='roc_auc', n_jobs=-1)\n",
    "    cv_scores_arr.append(cv_scores_)\n",
    "    \n",
    "    print(f\"sublinear_tf {i:<6}: {cv_scores_.mean():.4f}+-{cv_scores_.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_df 0.0001: 0.9581+-0.0129\n",
    "min_df 0.001: 0.9560+-0.0120\n",
    "min_df 0.01: 0.9403+-0.0132\n",
    "min_df 0.1: 0.9110+-0.0209\n",
    "min_df 0.3: 0.8979+-0.0200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_df 0.50: 0.9582+-0.0125\n",
    "max_df 0.60: 0.9583+-0.0126\n",
    "max_df 0.70: 0.9583+-0.0126\n",
    "max_df 0.80: 0.9583+-0.0126\n",
    "max_df 0.85: 0.9583+-0.0126\n",
    "max_df 0.90: 0.9586+-0.0127\n",
    "max_df 0.95: 0.9586+-0.0127\n",
    "max_df 0.98: 0.9586+-0.0127\n",
    "max_df 1.00: 0.8968+-0.0200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define good hour split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC: 0.9530+-0.0141\n"
     ]
    }
   ],
   "source": [
    "good_cols = ['sin_min', 'cos_max', 'n_null', 'dt', 'year', 'morning', 'day', 'evening']\n",
    "mask = pd.Series(ft_columns).isin(good_cols).values\n",
    "\n",
    "X_train = hstack([df_tf[:idx_split,:], full_time[:idx_split, mask]])\n",
    "X_test = hstack([df_tf[idx_split:,:], full_time[idx_split:, mask]])\n",
    "\n",
    "params = {'C': 0.1, 'class_weight': 'balanced', 'random_state':17, 'n_jobs':1}\n",
    "\n",
    "logit = LogisticRegression(**params)\n",
    "\n",
    "cv_scores = cross_val_score(logit, X_train, y_train, cv=kfold_split, \n",
    "                            scoring='roc_auc', n_jobs=-1) # hangs with n_jobs > 1, and locally this runs much faster\n",
    "\n",
    "print(f\"ROC_AUC: {cv_scores.mean():.4f}+-{cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bins 2         : 0.9465+-0.0194 EXCLUDE\n",
      "Bins 3         : 0.9418+-0.0187 EXCLUDE\n",
      "Bins 4         : 0.9583+-0.0126 ADD\n",
      "Bins 5         : 0.9494+-0.0208 EXCLUDE\n",
      "Bins 6         : 0.9429+-0.0222 EXCLUDE\n",
      "Bins 7         : 0.9548+-0.0194 EXCLUDE\n",
      "Bins 8         : 0.9592+-0.0168 ADD\n",
      "CPU times: user 1min 29s, sys: 34.6 s, total: 2min 4s\n",
      "Wall time: 20min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "good_cols = ['sin_min', 'cos_max', 'n_null', 'dt', 'year']\n",
    "mask = pd.Series(ft_columns).isin(good_cols).values\n",
    "\n",
    "mask_hours = pd.Series(ft_columns).isin(['hour']).values\n",
    "hours = pd.Series(full_time[:, mask_hours].flatten())\n",
    "\n",
    "n_cols = len(ft_columns)\n",
    "cv_scores_arr = []\n",
    "\n",
    "for i in range(2, 9):\n",
    "    hours_dum = pd.get_dummies(pd.cut(hours, bins=i)).values\n",
    "    \n",
    "    X_train = hstack([df_tf[:idx_split,:], full_time[:idx_split, mask], hours_dum[:idx_split, :]])\n",
    "    X_test = hstack([df_tf[idx_split:,:], full_time[idx_split:, mask], hours_dum[idx_split:, :]])\n",
    "    \n",
    "    logit = LogisticRegression(**params)\n",
    "    cv_scores_ = cross_val_score(logit, X_train, y_train, cv=kfold_split, scoring='roc_auc', n_jobs=-1)\n",
    "    cv_scores_arr.append(cv_scores_)\n",
    "    \n",
    "    d_cv = cv_scores_ - cv_scores\n",
    "    n_pos = (d_cv > 0).sum()\n",
    "    if not(d_cv.mean() > 0 and n_pos > 4):\n",
    "        print(f\"Bins {i:<10}: {cv_scores_.mean():.4f}+-{cv_scores_.std():.4f} EXCLUDE\")\n",
    "    else:\n",
    "        cv_scores = cv_scores_.copy()\n",
    "        print(f\"Bins {i:<10}: {cv_scores_.mean():.4f}+-{cv_scores_.std():.4f} ADD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add columns one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dow       : 0.9078+-0.0384 EXCLUDE\n",
      "weekend   : 0.9046+-0.0509 EXCLUDE\n",
      "day_      : 0.9058+-0.0344 EXCLUDE\n",
      "month     : 0.9166+-0.0213 EXCLUDE\n",
      "sin_min   : 0.9361+-0.0228 ADD\n",
      "cos_min   : 0.9349+-0.0238 EXCLUDE\n",
      "dt        : 0.9361+-0.0229 EXCLUDE\n",
      "dt_std    : 0.9361+-0.0229 EXCLUDE\n",
      "dt_mean   : 0.9361+-0.0229 EXCLUDE\n",
      "n_null    : 0.9362+-0.0228 ADD\n",
      "dt_time10 : 0.9362+-0.0229 ADD\n",
      "dt_time9  : 0.9362+-0.0229 EXCLUDE\n",
      "dt_time8  : 0.9362+-0.0229 EXCLUDE\n",
      "dt_time7  : 0.9362+-0.0229 EXCLUDE\n",
      "dt_time6  : 0.9362+-0.0229 EXCLUDE\n",
      "dt_time5  : 0.9362+-0.0229 EXCLUDE\n",
      "dt_time4  : 0.9362+-0.0229 EXCLUDE\n",
      "dt_time3  : 0.9362+-0.0229 EXCLUDE\n",
      "dt_time2  : 0.9362+-0.0229 EXCLUDE\n",
      "morning   : 0.9401+-0.0195 ADD\n",
      "day       : 0.9417+-0.0189 ADD\n",
      "evening   : 0.9418+-0.0188 ADD\n",
      "sin_max   : 0.9418+-0.0188 ADD\n",
      "cos_max   : 0.9483+-0.0172 ADD\n",
      "hour      : 0.9481+-0.0173 EXCLUDE\n",
      "year      : 0.9530+-0.0141 ADD\n",
      "minutes   : 0.9525+-0.0148 EXCLUDE\n",
      "CPU times: user 4min 55s, sys: 1min 50s, total: 6min 45s\n",
      "Wall time: 1h 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_cols = len(ft_columns)\n",
    "cv_scores_arr = []\n",
    "mask = np.zeros(n_cols, dtype='bool')\n",
    "for i in range(-n_cols, 0):\n",
    "    mask[i] = True\n",
    "    X_train = hstack([df_tf[:idx_split,:], full_time[:idx_split, mask]])\n",
    "    X_test = hstack([df_tf[idx_split:,:], full_time[idx_split:, mask]])\n",
    "    logit = LogisticRegression(**params)\n",
    "    cv_scores_ = cross_val_score(logit, X_train, y_train, cv=kfold_split, scoring='roc_auc', n_jobs=-1)\n",
    "    cv_scores_arr.append(cv_scores_)\n",
    "    \n",
    "    d_cv = cv_scores_ - cv_scores\n",
    "    n_pos = (d_cv > 0).sum()\n",
    "    if not(d_cv.mean() > 0 and n_pos > 4):\n",
    "        mask[i] = False\n",
    "        print(f\"{ft_columns[i]:<10}: {cv_scores_.mean():.4f}+-{cv_scores_.std():.4f} EXCLUDE\")\n",
    "    else:\n",
    "        cv_scores = cv_scores_.copy()\n",
    "        print(f\"{ft_columns[i]:<10}: {cv_scores_.mean():.4f}+-{cv_scores_.std():.4f} ADD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dow       : 0.9078+-0.0384 EXCLUDE  \n",
    "weekend   : 0.9046+-0.0509 EXCLUDE  \n",
    "day_      : 0.9058+-0.0344 EXCLUDE  \n",
    "month     : 0.9166+-0.0213 EXCLUDE  \n",
    "sin_min   : 0.9361+-0.0228 ADD  \n",
    "cos_min   : 0.9349+-0.0238 EXCLUDE  \n",
    "dt        : 0.9361+-0.0229 EXCLUDE  \n",
    "dt_std    : 0.9361+-0.0229 EXCLUDE  \n",
    "dt_mean   : 0.9361+-0.0229 EXCLUDE  \n",
    "n_null    : 0.9362+-0.0228 ADD  \n",
    "dt_time10 : 0.9362+-0.0229 ADD  \n",
    "dt_time9  : 0.9362+-0.0229 EXCLUDE  \n",
    "dt_time8  : 0.9362+-0.0229 EXCLUDE  \n",
    "dt_time7  : 0.9362+-0.0229 EXCLUDE  \n",
    "dt_time6  : 0.9362+-0.0229 EXCLUDE  \n",
    "dt_time5  : 0.9362+-0.0229 EXCLUDE  \n",
    "dt_time4  : 0.9362+-0.0229 EXCLUDE  \n",
    "dt_time3  : 0.9362+-0.0229 EXCLUDE  \n",
    "dt_time2  : 0.9362+-0.0229 EXCLUDE  \n",
    "morning   : 0.9401+-0.0195 ADD  \n",
    "day       : 0.9417+-0.0189 ADD  \n",
    "evening   : 0.9418+-0.0188 ADD  \n",
    "sin_max   : 0.9418+-0.0188 ADD  \n",
    "cos_max   : 0.9483+-0.0172 ADD  \n",
    "hour      : 0.9481+-0.0173 EXCLUDE   \n",
    "year      : 0.9530+-0.0141 ADD  \n",
    "minutes   : 0.9525+-0.0148 EXCLUDE  \n",
    "CPU times: user 4min 55s, sys: 1min 50s, total: 6min 45s  \n",
    "Wall time: 1h 2min 52s   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_cols = ['sin_min', 'cos_max', 'n_null', 'dt', 'year', 'morning', 'day', 'evening']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_      : -0.00000 N_pos: 2\n",
      "day       : -0.00000 N_pos: 4\n",
      "evening   : -0.00000 N_pos: 4\n",
      "minutes   : -0.00000 N_pos: 4\n",
      "dow       : -0.00012 N_pos: 0\n",
      "dt_time3  : -0.00004 N_pos: 1\n",
      "dt_time4  : -0.00001 N_pos: 2\n",
      "dt_time6  : -0.00017 N_pos: 2\n",
      "dt_time7  : -0.00013 N_pos: 3\n",
      "dt_time10 : -0.00303 N_pos: 3\n"
     ]
    }
   ],
   "source": [
    "cv_scores_arr_deleting = cv_scores_arr.copy()\n",
    "for i, cv_scores_ in enumerate(cv_scores_arr_deleting):\n",
    "    d_cv = cv_scores_ - cv_scores\n",
    "    n_pos = (d_cv > 0).sum()\n",
    "    if d_cv.mean() < 0 and n_pos < 5:\n",
    "        print(f\"{ft_columns[i]:<10}: {d_cv.mean():>8.5f} N_pos: {n_pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer df_tf + time  max_df=.7 sublinear_tf=True\n",
    "ROC_AUC: 0.9168+-0.0542\n",
    "Best params: {'C': 10, 'class_weight': {0: 0.6, 1: 0.4}}\n",
    "\n",
    "TfidfVectorizer df_tf + time  max_df=.7\n",
    "ROC_AUC: 0.9148+-0.0504\n",
    "Best params: {'C': 10, 'class_weight': {0: 0.6, 1: 0.4}}\n",
    "\n",
    "TfidfVectorizer df_tf + time  max_df=.8\n",
    "ROC_AUC: 0.9125+-0.0538\n",
    "Best params: {'C': 10, 'class_weight': {0: 0.6, 1: 0.4}}\n",
    "\n",
    "TfidfVectorizer df_tf + time\n",
    "ROC_AUC: 0.9115+-0.0546\n",
    "Best params: {'C': 10, 'class_weight': {0: 0.6, 1: 0.4}}\n",
    "\n",
    "TfidfVectorizer df_tf + time (sites name full)\n",
    "ROC_AUC: 0.9115+-0.0546\n",
    "Best params: {'C': 10, 'class_weight': {0: 0.6, 1: 0.4}}\n",
    "\n",
    "TfidfVectorizer df_tf\n",
    "ROC_AUC: 0.8575+-0.0753\n",
    "Best params: {'C': 20, 'class_weight': {0: 0.6, 1: 0.4}}\n",
    "\n",
    "CountVectorizer df_tf\n",
    "ROC_AUC: 0.8351+-0.0763\n",
    "Best params: {'C': 10, 'class_weight': {0: 0.6, 1: 0.4}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred = logit_grid_searcher.predict_proba(X_test)[:, 1]\n",
    "write_to_submission_file(logit_test_pred, 'pred/a_sub_new_df_tfidf_drop_dub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets watch origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User sessions are chosen in the way they are not longer than half an hour or/and contain more than ten websites. I.e. a session is considered as ended either if a user has visited ten websites or if a session has lasted over thirty minutes.\n",
    "\n",
    "There are some empty values in the table, it means that some sessions contain less than ten websites. Replace empty values with 0 and change columns types to integer. Also load the websites dictionary and check how it looks like:\n",
    "\n",
    "**Пользовательские сессии выбираются так, чтобы они не превышали получаса или / и содержали более десяти веб-сайтов. То есть сеанс считается завершенным, если пользователь посетил десять веб-сайтов или если сеанс длится более тридцати минут.**  \n",
    "\n",
    "**В таблице есть несколько пустых значений, это означает, что некоторые сеансы содержат менее десяти веб-сайтов. Замените пустые значения на 0 и измените типы столбцов на integer. Также загрузите словарь веб-сайтов и проверьте, как это выглядит:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-12 16:25:10</td>\n",
       "      <td>api.bing.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-12 16:25:11</td>\n",
       "      <td>api.bing.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-02-12 16:32:10</td>\n",
       "      <td>api.bing.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-12 16:32:11</td>\n",
       "      <td>www.google.fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-02-12 16:32:24</td>\n",
       "      <td>www.google.fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp           site\n",
       "0  2013-02-12 16:25:10   api.bing.com\n",
       "1  2013-02-12 16:25:11   api.bing.com\n",
       "2  2013-02-12 16:32:10   api.bing.com\n",
       "3  2013-02-12 16:32:11  www.google.fr\n",
       "4  2013-02-12 16:32:24  www.google.fr"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alice = pd.read_csv('data/train/Alice_log.csv')\n",
    "df_alice.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-18 10:19:27</td>\n",
       "      <td>ocsp.digicert.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-18 10:19:28</td>\n",
       "      <td>ocsp.digicert.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-18 10:19:28</td>\n",
       "      <td>clients1.google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-18 10:19:29</td>\n",
       "      <td>gtglobal-ocsp.geotrust.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-18 10:19:29</td>\n",
       "      <td>clients1.google.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                        site\n",
       "0  2013-12-18 10:19:27           ocsp.digicert.com\n",
       "1  2013-12-18 10:19:28           ocsp.digicert.com\n",
       "2  2013-12-18 10:19:28         clients1.google.com\n",
       "3  2013-12-18 10:19:29  gtglobal-ocsp.geotrust.com\n",
       "4  2013-12-18 10:19:29         clients1.google.com"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user0010 = pd.read_csv('data/train/other_user_logs/user0010.csv')\n",
    "df_user0010.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>site6</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>952</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "21669          56 2013-01-12 08:05:57     55 2013-01-12 08:05:57      0   \n",
       "54843          56 2013-01-12 08:37:23     55 2013-01-12 08:37:23     56   \n",
       "77292         946 2013-01-12 08:50:13    946 2013-01-12 08:50:14    951   \n",
       "114021        945 2013-01-12 08:50:17    948 2013-01-12 08:50:17    949   \n",
       "146670        947 2013-01-12 08:50:20    950 2013-01-12 08:50:20    948   \n",
       "\n",
       "                         time3  site4               time4  site5  \\\n",
       "session_id                                                         \n",
       "21669                      NaT      0                 NaT      0   \n",
       "54843      2013-01-12 09:07:07     55 2013-01-12 09:07:09      0   \n",
       "77292      2013-01-12 08:50:15    946 2013-01-12 08:50:15    946   \n",
       "114021     2013-01-12 08:50:18    948 2013-01-12 08:50:18    945   \n",
       "146670     2013-01-12 08:50:20    947 2013-01-12 08:50:21    950   \n",
       "\n",
       "                         time5  site6               time6  site7  \\\n",
       "session_id                                                         \n",
       "21669                      NaT      0                 NaT      0   \n",
       "54843                      NaT      0                 NaT      0   \n",
       "77292      2013-01-12 08:50:16    945 2013-01-12 08:50:16    948   \n",
       "114021     2013-01-12 08:50:18    946 2013-01-12 08:50:18    947   \n",
       "146670     2013-01-12 08:50:21    952 2013-01-12 08:50:21    946   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "21669                      NaT      0                 NaT      0   \n",
       "54843                      NaT      0                 NaT      0   \n",
       "77292      2013-01-12 08:50:16    784 2013-01-12 08:50:16    949   \n",
       "114021     2013-01-12 08:50:19    945 2013-01-12 08:50:19    946   \n",
       "146670     2013-01-12 08:50:21    951 2013-01-12 08:50:22    946   \n",
       "\n",
       "                         time9  site10              time10  \n",
       "session_id                                                  \n",
       "21669                      NaT       0                 NaT  \n",
       "54843                      NaT       0                 NaT  \n",
       "77292      2013-01-12 08:50:17     946 2013-01-12 08:50:17  \n",
       "114021     2013-01-12 08:50:19     946 2013-01-12 08:50:20  \n",
       "146670     2013-01-12 08:50:22     947 2013-01-12 08:50:22  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2297 entries, 251175 to 244233\n",
      "Data columns (total 21 columns):\n",
      "site1     2297 non-null int32\n",
      "time1     2297 non-null datetime64[ns]\n",
      "site2     2297 non-null int32\n",
      "time2     2294 non-null datetime64[ns]\n",
      "site3     2297 non-null int32\n",
      "time3     2287 non-null datetime64[ns]\n",
      "site4     2297 non-null int32\n",
      "time4     2286 non-null datetime64[ns]\n",
      "site5     2297 non-null int32\n",
      "time5     2280 non-null datetime64[ns]\n",
      "site6     2297 non-null int32\n",
      "time6     2273 non-null datetime64[ns]\n",
      "site7     2297 non-null int32\n",
      "time7     2269 non-null datetime64[ns]\n",
      "site8     2297 non-null int32\n",
      "time8     2263 non-null datetime64[ns]\n",
      "site9     2297 non-null int32\n",
      "time9     2262 non-null datetime64[ns]\n",
      "site10    2297 non-null int32\n",
      "time10    2258 non-null datetime64[ns]\n",
      "target    2297 non-null int64\n",
      "dtypes: datetime64[ns](10), int32(10), int64(1)\n",
      "memory usage: 305.1 KB\n"
     ]
    }
   ],
   "source": [
    "train_df[train_df['target']==1].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "      <th>timestamp2</th>\n",
       "      <th>site2</th>\n",
       "      <th>timestamp3</th>\n",
       "      <th>site3</th>\n",
       "      <th>timestamp4</th>\n",
       "      <th>site4</th>\n",
       "      <th>timestamp5</th>\n",
       "      <th>site5</th>\n",
       "      <th>timestamp6</th>\n",
       "      <th>site6</th>\n",
       "      <th>timestamp7</th>\n",
       "      <th>site7</th>\n",
       "      <th>timestamp8</th>\n",
       "      <th>site8</th>\n",
       "      <th>timestamp9</th>\n",
       "      <th>site9</th>\n",
       "      <th>timestamp10</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-12 16:25:10</td>\n",
       "      <td>270</td>\n",
       "      <td>2013-02-12 16:25:11</td>\n",
       "      <td>270.0</td>\n",
       "      <td>2013-02-12 16:32:10</td>\n",
       "      <td>270.0</td>\n",
       "      <td>2013-02-12 16:32:11</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:24</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:26</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>7832.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-12 16:25:11</td>\n",
       "      <td>270</td>\n",
       "      <td>2013-02-12 16:32:10</td>\n",
       "      <td>270.0</td>\n",
       "      <td>2013-02-12 16:32:11</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:24</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:26</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-02-12 16:32:10</td>\n",
       "      <td>270</td>\n",
       "      <td>2013-02-12 16:32:11</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:24</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:26</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2013-02-12 16:32:28</td>\n",
       "      <td>7832.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-12 16:32:11</td>\n",
       "      <td>21</td>\n",
       "      <td>2013-02-12 16:32:24</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:26</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2013-02-12 16:32:28</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:29</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-02-12 16:32:24</td>\n",
       "      <td>21</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2013-02-12 16:32:26</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2013-02-12 16:32:28</td>\n",
       "      <td>7832.0</td>\n",
       "      <td>2013-02-12 16:32:29</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2013-02-12 16:32:34</td>\n",
       "      <td>7832.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  site          timestamp2   site2          timestamp3  \\\n",
       "0 2013-02-12 16:25:10   270 2013-02-12 16:25:11   270.0 2013-02-12 16:32:10   \n",
       "1 2013-02-12 16:25:11   270 2013-02-12 16:32:10   270.0 2013-02-12 16:32:11   \n",
       "2 2013-02-12 16:32:10   270 2013-02-12 16:32:11    21.0 2013-02-12 16:32:24   \n",
       "3 2013-02-12 16:32:11    21 2013-02-12 16:32:24    21.0 2013-02-12 16:32:25   \n",
       "4 2013-02-12 16:32:24    21 2013-02-12 16:32:25  7832.0 2013-02-12 16:32:25   \n",
       "\n",
       "    site3          timestamp4   site4          timestamp5   site5  \\\n",
       "0   270.0 2013-02-12 16:32:11    21.0 2013-02-12 16:32:24    21.0   \n",
       "1    21.0 2013-02-12 16:32:24    21.0 2013-02-12 16:32:25  7832.0   \n",
       "2    21.0 2013-02-12 16:32:25  7832.0 2013-02-12 16:32:25    21.0   \n",
       "3  7832.0 2013-02-12 16:32:25    21.0 2013-02-12 16:32:26  7832.0   \n",
       "4    21.0 2013-02-12 16:32:26  7832.0 2013-02-12 16:32:27    30.0   \n",
       "\n",
       "           timestamp6   site6          timestamp7   site7          timestamp8  \\\n",
       "0 2013-02-12 16:32:25  7832.0 2013-02-12 16:32:25    21.0 2013-02-12 16:32:26   \n",
       "1 2013-02-12 16:32:25    21.0 2013-02-12 16:32:26  7832.0 2013-02-12 16:32:27   \n",
       "2 2013-02-12 16:32:26  7832.0 2013-02-12 16:32:27    30.0 2013-02-12 16:32:27   \n",
       "3 2013-02-12 16:32:27    30.0 2013-02-12 16:32:27  7832.0 2013-02-12 16:32:27   \n",
       "4 2013-02-12 16:32:27  7832.0 2013-02-12 16:32:27    29.0 2013-02-12 16:32:28   \n",
       "\n",
       "    site8          timestamp9   site9         timestamp10  site10  \n",
       "0  7832.0 2013-02-12 16:32:27    30.0 2013-02-12 16:32:27  7832.0  \n",
       "1    30.0 2013-02-12 16:32:27  7832.0 2013-02-12 16:32:27    29.0  \n",
       "2  7832.0 2013-02-12 16:32:27    29.0 2013-02-12 16:32:28  7832.0  \n",
       "3    29.0 2013-02-12 16:32:28  7832.0 2013-02-12 16:32:29    37.0  \n",
       "4  7832.0 2013-02-12 16:32:29    37.0 2013-02-12 16:32:34  7832.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT = 30\n",
    "\n",
    "df_alice = pd.read_csv('data/train/Alice_log.csv')\n",
    "df_alice['timestamp'] = df_alice['timestamp'].apply(pd.to_datetime)\n",
    "for i in range(-1, -10, -1):\n",
    "    df_alice['timestamp' + str(-i+1)] = df_alice['timestamp'].shift(i)\n",
    "    df_alice['site' + str(-i+1)] = df_alice['site'].shift(i)\n",
    "    \n",
    "    df_alice['dt'] = (df_alice['timestamp' + str(-i+1)] - df_alice['timestamp']).dt.seconds / 60\n",
    "    df_alice.loc[df_alice['dt']>DT, ['timestamp' + str(-i+1), 'site' + str(-i+1)]] = None\n",
    "\n",
    "del df_alice['dt']\n",
    "\n",
    "to_int = dict(zip(sites_dict['site'], sites_dict.index))\n",
    "for col in df_alice.columns:\n",
    "    if 'site' in col:\n",
    "        df_alice[col] = df_alice[col].map(to_int)\n",
    "\n",
    "# df_alice['dt'] = (df_alice['timestamp'].shift(-1) - df_alice['timestamp']).dt.seconds / 60\n",
    "# df_alice['dt_10sites'] = (df_alice['timestamp'].shift(-10) - df_alice['timestamp']).dt.seconds / 60\n",
    "df_alice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
